{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30279bd3-481b-4ca7-a20b-350d8334ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3efcc9-a243-4d5a-b3cd-45eb4b10e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "print(titanic.shape)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1d6cf-6ce1-413c-8aac-249765c7a3d6",
   "metadata": {},
   "source": [
    "The `Titanic` data set collects information about almost 900 passangers aboard the Titanic during the fateful voyage when it crashed into an iceberg in 1912 and sank. The information includes their age; the fare they paid for their ticket (in British pounds); their sex; and the passenger class `Pclass`, with 1st class corresponding to VIP treatment and 3rd class corresponding to a much less luxurious experience. Crucially, the data set also records whether that passenger survived the sinking of the ship, with `1` indicating that the passenger survived and `0` indicating that the passenger tragically perished.\n",
    "\n",
    "We are eventually going to train an algorithm to predict whether a passenger survived the Titanic based on their available information. Before we do, let's get a sense for some trends using familiar pandas summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235a492-dcdd-406c-af5d-c349609ba5f5",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa135d-3c73-4785-adc5-556dcc4ce1f0",
   "metadata": {},
   "source": [
    "How wealthy were these passengers? We can't know for certain, but we can get a sense for how much was paid for each passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583a78d-57a8-42f9-be18-9fcce724da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Pclass').mean(numeric_only=True)\n",
    "\n",
    "# group everyone by passenger class, and calculate mean "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c4d14-282d-4eb2-9589-ff69c8b560da",
   "metadata": {},
   "source": [
    "- The average price of 84 pounds for a first-class ticket corresponds to nearly \\$15,000 USD today.\n",
    "- The second-class ticket corresponds to roughly \\$3,500.\n",
    "- The third class ticket corresponds to roughly \\$2,500.\n",
    "\n",
    "We can safely assume that the first-class passengers were indeed substantially more wealthy on average than the others.\n",
    "\n",
    "Did Pclass have an effect on survival rate?\n",
    "\n",
    "This difference in wealth made a considerable difference in how likely passengers were to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1334ab-2555-4ee4-a8ac-ce876e68447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'save the women first' (and maybe children, TBD)\n",
    "\n",
    "titanic.groupby(['Pclass', 'Sex']).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efdcf8-766b-40a6-8dec-aee0b539fbb6",
   "metadata": {},
   "source": [
    "This table reflects the famous maritime tradition of prioritizing women and children first into the lifeboats, resulting in vastly higher survival rates among women in these data. Note the role of class: a 1st-class woman was twice as likely to survive as a third class woman, and a 1st-class man was nearly three times as likely to survive as a 3rd class man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8f771-bae2-42c6-ba7d-b361908d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristics of who survived and who didn't\n",
    "\n",
    "titanic.groupby('Survived').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a04359-2458-4c25-b0a8-208916cd9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Survived').Sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5948f6-dc8a-4716-8746-7c17a49ac3fd",
   "metadata": {},
   "source": [
    "# Our first ML algorithm (classification)\n",
    "\n",
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc5150-3295-4d42-80e6-10e3cc9ea74a",
   "metadata": {},
   "source": [
    "We'd like to develop automated models that can use these trends and others to make predictions about survival. However, we need to do a bit of data cleaning before we're ready for this. In particular, machine learning algorithms don't really get text, so we need to transform text data into numbers before we can proceed. Here's how we can encode the `Sex` data. \n",
    "\n",
    "\n",
    "We also don't really have any use for the actual names of passengers, so let's just remove that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed6e25-a894-4c21-8c1c-7cc455f51f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.drop(columns=['Name']) # drop the column 'Name'\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88241927-4c6e-48dd-b454-7a41c86494fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn Sex column into numbers\n",
    "is_F = (titanic['Sex'] == 'female') # array of True and False\n",
    "titanic['Sex'] = is_F.astype(int) # 1 = female, 0 = male\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49665c9d-14e2-4756-9faf-9a8f80ed3852",
   "metadata": {},
   "source": [
    "Now in an ideal world, we would find the best model or classifier using all the data we have (train), then go find future or unseen data (test), then try making predictions on the new test data. In this case, that's not only impossible but also a weird concept (what's the point of making really good predictions on an event that happened a century ago?). But we do need to find a way to check if our model is actually good at predicting.\n",
    "\n",
    "In cases like this (which is very common), people **randomly split their samples into two groups, and reserve the samples in the test set for evaluating the model**. \n",
    "\n",
    "Think of it as a professor reserving some questions in the question bank for the actual test (test) and releasing the rest as practice questions (train). Why would giving all the questions ahead of time be an inaccurate way to evaluate a student's understanding (model)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f7a05-915b-478a-8641-088536d6307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = titanic.sample(frac= ) # 80% rows for training\n",
    "test = titanic.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f889f-74a6-4c07-a931-a0d713a3d3a7",
   "metadata": {},
   "source": [
    "The next thing to do is to separate out the target data `Survived` from the predictor data (everything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2e86e-e37d-4bcd-90b1-92493b91762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[        ]\n",
    "X_train = train.drop(columns=[        ])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test[        ]\n",
    "X_test = test.drop(columns=[        ]) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f1de5-4b1d-4fcc-8dc3-4c37a6ce79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is from last lecture demo\n",
    "# y = titanic['Survived']\n",
    "# X = titanic.drop(columns=['Survived'])\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7adb5-97c4-401f-9615-4f97918ba99d",
   "metadata": {},
   "source": [
    "## Training or fitting a model \n",
    "\n",
    "To use a machine learning model from `scikit-learn`, you should import the relevant model. For example, let's use a decision tree classifier.\n",
    "\n",
    "Arguments passed to the model upon instantiation (`max_depth`=2) are typically used to control how complex the model can be. These arguments are often referred to as hyperparameters. In practice, we don't usually know what the right hyperparameters are, and so we need to resort to various computational techniques (in coming lectures) to select good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a59769-1d00-4e90-90b5-8360edbff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "# sklearn is a package called scikit-learn \n",
    "# that contains a lot of useful ML algorithms\n",
    "\n",
    "T = tree.DecisionTreeClassifier(max_depth=2) \n",
    "\n",
    "T.fit(        ,         ) # find the best f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8103725-6019-472c-9daa-a224a1942524",
   "metadata": {},
   "source": [
    "X -> [model] -> f(X)\n",
    "\n",
    "What does the score function calculate? It depends by model, but in the case of classifiers, the score is the fraction of the time that the model made the correct prediction. The score does the same job as the loss function, but the signs are flipped -- high scores mean good models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246acd0-e9c4-486f-acc7-df089af02b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.score(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25179c16-6272-4183-8bcd-66aa819668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.score(X_test, y_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f405625-a646-441b-a75f-1e2be9a57549",
   "metadata": {},
   "source": [
    "Our model was able to use the predictor variables to be right nearly 80% of the time. That's pretty impressive, but there is an important problem here. When doing machine learning, it's not advised to score or evaluate your model on the same data used for training or fitting the model. We'll come back to this in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f50f49-f883-482b-aeeb-3810e02dda3d",
   "metadata": {},
   "source": [
    "In most cases, you should aim to understand how your model makes the decisions that it does. This is the problem of machine learning interpretation.\n",
    "\n",
    "Every machine learning algorithm has both strengths and weaknesses when it comes to interpretation. Decision trees are pretty pleasant to interpret, as they correspond to \"flow-chart\" style reasoning that many of us are familiar with. The `tree` module of `scikit-learn` provides a convenient method for visualizing decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed6379-6f72-4d7c-9de9-6254b2ad9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "p = tree.plot_tree(T, \n",
    "                   filled=True, \n",
    "                   feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb583759-40cd-4bee-b7d1-b7620b8e5b2a",
   "metadata": {},
   "source": [
    "The automated decision tree classifier has found the following rule to predict whether a passenger survived the Ttianic. First, check whether Sex <= 0.5 -- that is, check whether the passenger is male (0) or female (1).\n",
    "\n",
    "- If the passenger is male, then check how old they are. If they are young, the algorithm gives them a fair chance of survival (remember: \"women and children.\"). Otherwise, the algorithm gives them very low odds indeed.\n",
    "\n",
    "- If so, next check whether the passenger was first or second-class. If so, then they survive with high probability; otherwise the algorithm isn't sure and gives them a roughly 50-50 chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7b64c-8b1a-4282-a0a3-37a6ec78130f",
   "metadata": {},
   "source": [
    "We've now gone through one cycle of:\n",
    "\n",
    "1. Acquiring data.\n",
    "2. Exploratory analysis.\n",
    "3. Modeling.\n",
    "4. Interpreting results.\n",
    "\n",
    "In practice, we would not stop here. Having trained our model, we should then ask:\n",
    "\n",
    "- What new insights can we gain from our model about the underlying data set?\n",
    "- Can we improve our model?\n",
    "- How can we use what we have learned in other data sets?\n",
    "\n",
    "These questions, and many others, are part of the cycle of data science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e26e99-7af7-4dd7-832b-6b9b2d50e5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350da7c-cdc7-410a-b5a6-b320bfef0d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71dbc6f3-6222-40ba-9895-08f5be51afad",
   "metadata": {},
   "source": [
    "if we have time... regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10360291-9df6-4303-a5c3-8bdf9b7adaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls random number generation\n",
    "# always get the same data\n",
    "np.random.seed(1234) \n",
    "\n",
    "# true model is linear with a = 1 and b = 1\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "n_points = 100\n",
    "\n",
    "X = np.random.rand(n_points)\n",
    "Y = a*X + b + 0.2*np.random.randn(n_points) # final term is random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915676bc-9bc9-471d-8e91-599b67f83398",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.plot([0,1], [1, 2], color = \"black\", label = \"true model\")\n",
    "ax.plot([0,1], [1.2, 2.2], color = \"black\", linestyle=':', label = \"another model 1\")\n",
    "ax.plot([0,1], [0.8, 2.2], color = \"black\", linestyle='-.', label = \"another model 2\")\n",
    "ax.scatter(X, Y, label = \"data\")\n",
    "ax.set(xlabel='X', ylabel='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f180c96-b5f7-4be3-a8ae-8b1dea828bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
