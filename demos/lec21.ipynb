{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the temperament of ROUSes using k-NN classification\n",
    "Using other data we have in the table, we want to predict the temperament of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "First, let's look at a scatterplot with the temperament represented as color and symbols to get a general idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Age',y='Length', hue='Temperament', style='Temperament')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are some clusters of the same temperament, which means the samples have the same temperament as their neighbors, so k-NN should work well for those.  But there are also definitely some samples are more \"alone\" so k-NN won't be as good for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize columns\n",
    "The next cell normalizes the columns so the neighbor distance calculations will be scaled equivalently.  You can try skipping this cell to see the performance without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try skipping this cell and see results without scaling\n",
    "rouses['Age'] = (rouses['Age']-rouses['Age'].min())/( rouses['Age'].max()-rouses['Age'].min()) # normalize 'Age' columns\n",
    "rouses['Length'] = (rouses['Length']-rouses['Length'].min())/( rouses['Length'].max()-rouses['Length'].min()) # normalize 'Length' columns\n",
    "rouses['Weight'] = (rouses['Weight']-rouses['Weight'].min())/( rouses['Weight'].max()-rouses['Weight'].min()) # normalize 'Weight' columns\n",
    "\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, as usual let's follow the train and test process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=1234) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Temperament']\n",
    "X_train = train.drop(columns=['Temperament'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Temperament']\n",
    "X_test = test.drop(columns=['Temperament']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that is a very small number of train and test samples, so our results are going to be highly dependent on how the data is split.  Let's try k-NN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)\n",
    "print('Prediction:',knn.predict(X_test))\n",
    "print('Actual:',list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, predicting 4 out of 6 isn't great, but it is actually better than expected considering the Train score.  Predicting temperament is a pretty tough challenge!  Try different values for k (`n_neighbors`) to see what changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the weight of ROUSes using k-NN\n",
    "Using other data we have in the table, we want to predict the weight of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Length',y='Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous linear regression work, we were able to use `Age` to predict `Weight` quite well because the correlation was close to linear.  Let's try using `Length` instead, and see how well we can predict despite the relationship being less linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = rouses.drop(columns=['Temperament','Age']) # drop the columns 'Temperament' and 'Age'\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=4321) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to separate out the target data `Weight` from the predictor data (everything else; in this case just `Length` is left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Weight']\n",
    "X_train = train.drop(columns=['Weight'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Weight']\n",
    "X_test = test.drop(columns=['Weight']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's try using weighted K-NN for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression a \"score\" (the R2 value) near 1 is what we are hoping for, and 0 is the worst result.  So our model is doing a very good job at predicting the data!\n",
    "\n",
    "To visualize, we can plug in the test values in and have their outputs predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(train,x='Length', y='Weight', label = \"training data\")\n",
    "ax.scatter(test['Length'], predictions, label = \"test\")\n",
    "ax.set(xlabel='Length', ylabel='Weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it looks like these predictions follow the trend of the data! You can try different values for k to see how the results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "\n",
    "## Predicting the weight using length and temperament.\n",
    "\n",
    "Let's try adding in `Temperament` along with `Length` to help predict `Weight`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('ROUSes.csv')\n",
    "rouses=rouses.drop(columns=['Age'])\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how weight varies for different `Temperament`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=rouses, x='Weight',y='Temperament')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, `Temperament` might not much help to predict `Weight`!  That range of `Weight` for the `Sleepy` category means that `Temperament` won't be a very good predictor for `Weight` for ROUSes in that category, and it might actually end up hurting our predictions.  But the tighter range for `No-nonsense` is more promising.  \n",
    "\n",
    "As we learned in lecture, to do distance calculations for a categorical feature we'll use one-hot encoding to convert `Temperament` to 4 new columns.  Also, we need to normalize the `Length` feature to make similar scales.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses=pd.get_dummies(rouses,dtype=int)\n",
    "rouses['Length']= (rouses['Length']-rouses['Length'].min())/(rouses['Length'].max()-rouses['Length'].min()) # normalize Length\n",
    "rouses.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So easy!  Okay, let's do the prediction for `Weight` again, now using `Length` and `Temperament` one-hot features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=4321) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Weight']\n",
    "X_train = train.drop(columns=['Weight'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Weight']\n",
    "X_test = test.drop(columns=['Weight']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(train,x='Length', y='Weight', label = \"training data\")\n",
    "ax.scatter(test['Length'], predictions, label = \"predictions\")\n",
    "ax.set(xlabel='Length', ylabel='Weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it looks like adding the `Temperament` feature actually hurt prediction performance.  Unlike with Linear Regression, there are no coefficients to change the importance of each feature, so it is very important to choose useful features for k-NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
